{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization with T5 on XSum\n",
    "\n",
    "We are going to fine-tune the [T5 model, implemented by HuggingFace](https://huggingface.co/t5-small), for text summarization on the [Extreme Summarization (XSum)](https://huggingface.co/datasets/xsum) dataset.\n",
    "The data is composed by news articles and the corresponding summaries.\n",
    "\n",
    "We will be using the following model sizes available from HuggingFace\n",
    "\n",
    "| Variant                                     |   Parameters    |\n",
    "|:-------------------------------------------:|----------------:|\n",
    "| [T5-small](https://huggingface.co/t5-small) |    60,506,624   | \n",
    "| [T5-large](https://huggingface.co/t5-large) |   737,668,096   | \n",
    "| [T5-3b](https://huggingface.co/t5-3b)       | 2,851,598,336   | \n",
    "\n",
    "\n",
    "More info:\n",
    "* This notebooks is based on the script [run_summarization_no_trainer.py](https://github.com/huggingface/transformers/blob/v4.12.5/examples/pytorch/summarization/run_summarization_no_trainer.py) from HuggingFace\n",
    "* [T5 on HuggingFace docs](https://huggingface.co/transformers/model_doc/t5.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/daint/UES/6.0.UP04/sandboxes/sarafael/hpcpython2022/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-10-13 13:00:59.378664: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-13 13:00:59.708382: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-13 13:01:01.744177: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/daint/UES/6.0.UP04/sandboxes/sarafael/software/cuDNN/8.1.0/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/compat:/usr/local/cuda-11.3/compat:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/math_libs/11.3/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/lib64:/usr/local/cuda-11.0/lib64:/opt/cray/pe/mpt/7.7.18/gni/mpich-gnu/8.2/lib:/opt/cray/pe/perftools/21.09.0/lib64:/opt/cray/rca/2.2.20-7.0.3.1_3.15__g8e3fb5b.ari/lib64:/opt/cray/pe/pmi/5.0.17/lib64:/opt/cray/pe/libsci/20.09.1/GNU/8.1/x86_64/lib:/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/graphviz/2.50.0-CrayGNU-21.09/lib:/opt/cray/pe/libsci_acc/20.10.1/GNU/8.1/x86_64/lib:/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/Julia/1.6.3-CrayGNU-21.09-cuda/lib:/opt/python/3.9.4.1/lib:/opt/cray/pe/gcc-libs:/opt/cray/pe/papi/6.0.0.9/lib64:/opt/gcc/9.3.0/snos/lib64\n",
      "2022-10-13 13:01:01.746997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/daint/UES/6.0.UP04/sandboxes/sarafael/software/cuDNN/8.1.0/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/compat:/usr/local/cuda-11.3/compat:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/math_libs/11.3/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/lib64:/usr/local/cuda-11.0/lib64:/opt/cray/pe/mpt/7.7.18/gni/mpich-gnu/8.2/lib:/opt/cray/pe/perftools/21.09.0/lib64:/opt/cray/rca/2.2.20-7.0.3.1_3.15__g8e3fb5b.ari/lib64:/opt/cray/pe/pmi/5.0.17/lib64:/opt/cray/pe/libsci/20.09.1/GNU/8.1/x86_64/lib:/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/graphviz/2.50.0-CrayGNU-21.09/lib:/opt/cray/pe/libsci_acc/20.10.1/GNU/8.1/x86_64/lib:/apps/daint/UES/jenkins/7.0.UP03/21.09/daint-gpu/software/Julia/1.6.3-CrayGNU-21.09-cuda/lib:/opt/python/3.9.4.1/lib:/opt/cray/pe/gcc-libs:/opt/cray/pe/papi/6.0.0.9/lib64:/opt/gcc/9.3.0/snos/lib64\n",
      "2022-10-13 13:01:01.747011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/users/class424/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = load_dataset('xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 188948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15575668'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['id'][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BB King was hailed as one of the greatest blues musicians of all time.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['summary'][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'His vibrato style of playing influenced a generation of rock and blues guitarists, including Eric Clapton, Mike Bloomfield and Stevie Ray Vaughan.\\nRolling Stone magazine once ranked BB King in third place in its list of the 100 greatest guitarists of all time, just below Jimi Hendrix and Duane Allman.\\nHis output crossed musical barriers, from jazz and blues to mainstream pop.\\nHe was born Riley B King in Indianola, Mississippi, on 16 September 1925. His parents were sharecroppers and, as a young boy, he helped them work in the fields.\\nThe family struggled. \"When you live in a house that you can always peek out of and see what kind of day it is,\" King later said, \"you\\'re not doing so well.\"\\nThe sound of his co-workers hollering the blues was his first introduction to the style of music that he was to help take from a purely black American audience into the mainstream.\\nHe bought his first guitar when he was barely a teenager so he could play at church services. In 1947 he moved to Memphis where he busked on the streets until he found work as a radio disc jockey at station WDIA.\\nHe was introduced as \"The Beale Street Blues Boy\", later shortened to BB. He also built a reputation as a guitarist in the Beale Street blues clubs.\\nHe later said: \"I\\'ve said that playing the blues is like having to be black twice.\"\\nIt was while playing in one of the clubs that a fight broke out over a woman, causing a fire. After rushing out of the wooden building, he realised that he had left his guitar behind.\\nHe risked his life by going back in to rescue his instrument. He named it after the woman whose charms had been behind the trouble: Lucille.\\nAfter making his first record in 1949, he went on to top the rhythm and blues charts two years later with Three O\\'Clock Blues. The song remained at number one for 17 weeks.\\nMany of his early recordings were produced by the legendary Sam Phillips who went on to found Sun Records.\\nOn the strength of this success, he was able to work across the US and he performed at such venues as the Apollo Theatre in Harlem, New York. Further hits included Sweet Black Angel, Rock Me Baby and Every Day I Have the Blues.\\nHe played more than 300 gigs on the so-called Chitlin\\' Circuit, the collection of performance venues in what were then racially segregated southern states where it was safe for black musicians to perform.\\nKing said: \"I have put up with more humiliation than I care to remember.\\n\"Touring a segregated America, forever being stopped and harassed by white cops hurt you most \\'cos you didn\\'t realise the damage. You hold it in.\"\\nIt was thanks to the influence of British bands such as the Yardbirds, the Animals and the Rolling Stones that white audiences, first in the UK and later in America, began to embrace the blues.\\nBB King began to be accepted in venues that had long been closed to black musicians. One of his more moving moments was when he was given a standing ovation by a mainly white audience at the Fillmore West theatre in San Francisco in 1968.\\nHe later recalled that he had berated his bus driver for bringing him to the wrong venue after seeing the overwhelmingly white faces in the queues of people waiting to get in.\\nThe same year, he made his first tour of Europe. He returned many times, becoming as popular there as at home.\\nHe had a UK top 20 hit with The Thrill is Gone in 1969, but his most successful single came with the band U2 in 1989 with When Love Comes To Town.\\nIn 2000 he collaborated with long-time fan, and blues purist, Eric Clapton on the album Riding with the King.\\nKing returned to Mississippi each year to visit his numerous children from a number of relationships.\\nHe once said: \"Ladies, friends and music - without those three, I wouldn\\'t wanna be here.\"\\nKing was still touring in his 80s, having played more than 15,000 live gigs during his career.\\nHe also made a point of playing regular concerts in prisons across the US.\\nKing was once asked what motivated him to keep up his punishing schedule of live performances.\\n\"I would like very much to make them happy,\" he replied. \"I want them, when they leave the venue, to say \\'I enjoyed myself\\'.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train']['document'][sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 1.28MB/s]\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 1.32MB/s]\n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 2.29MB/s]\n",
      "/apps/daint/UES/6.0.UP04/sandboxes/sarafael/hpcpython2022/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hf_model = 't5-small'\n",
    "t5_cache = os.path.join(os.getcwd(), 'cache')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    hf_model,\n",
    "    use_fast=True,\n",
    "    cache_dir=os.path.join(t5_cache, f'{hf_model}_tokenizer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(\"What's up tokenizer!\",\n",
    "                         max_length=1024,\n",
    "                         padding=False,\n",
    "                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [363, 31, 7, 95, 14145, 8585, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * `attention_mask` indicates what's text and what's padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', \"'\", 's', 'up', 'token', 'izer', '!', '</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(encoded_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    encoded_text = tokenizer(\"What's up tokenizer!\", max_length=1024,\n",
    "                             padding=False, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [363, 31, 7, 95, 14145, 8585, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):    \n",
    "    inputs = examples['document']\n",
    "    targets = examples['summary']\n",
    "    inputs = [f'summarize: {inp}' for inp in inputs]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=1024,\n",
    "                             padding=False, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128,\n",
    "                           padding=False, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 978 ms, sys: 328 ms, total: 1.31 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_datasets = hf_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=hf_dataset[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    num_proc=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training Sequence to Sequence models, we need a special kind of data collator,\n",
    "# which will not only pad the inputs to the maximum length in the batch,\n",
    "# but also the labels.\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    label_pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "per_device_train_batch_size = 128\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=per_device_train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    if step > 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21603,    10,    37,  ...,     0,     0,     0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']  # indicates what's text and what's padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: The 39-year-old former world number one is now ranked 96 in the world and without a PGA Tour title since 2012. But his fifth birdie gave him a four-under 67 and took him to 10 under alongside Canadian Graham DeLaet. Ian Poulter enhanced his quest for a PGA Tour card with a 68 to earn a share of third place, two strokes back. The 41-year-old is playing the penultimate event of his 10-tournament medical exemption and will secure his card with 12th place or better. Find out how to get into golf with our special guide. He had five birdies to reach eight under at the Harbour Town links. Donald, four times a runner-up in the tournament, had three consecutive birdies on the front nine and a superb bunker shot to two feet from a precarious plugged lie helped him to save par at the 17th. He then produced a delicate lofted chip from the right of the 18th fairway that checked and trickled into the cup. \"I\\'ve always felt like I pitch the ball really well round here,\" said Donald. \"The grass lends itself to being able to create some spin and I needed it down there - I didn\\'t have much green to work with - it just came off perfectly with a little side spin into the cup. \"I\\'ve hit a few poor drives today and I need to work on that. It was a little bit more stressful than I would have liked but a great short game bailed me out.\" De Laet, the world number 128, is still to win on the PGA Tour and has missed four cuts this season, but he eagled the par-four ninth en route to a 67. Danny Willett, who missed the cut in his defence of the Masters at Augusta, had two double bogeys in a 78 that left him at seven over, and absent for the weekend for a second tournament in succession. Another Sheffield golfer, Matt Fitzpatrick, who was 32nd at Augusta, also missed the cut after four bogeys and a double bogey in a 72 for a three-over total. But Scotsman Russell Knox, who missed the cut at Augusta, had four birdies in five holes in a 66 to reach six under, one ahead of England\\'s Tyrrell Hatton. Andrew \\'Beef\\' Johnston, who has missed the cut in three of his six PGA Tour events this season, is three under after a 71. Northern Ireland\\'s Graeme McDowell, winner of this event after a play-off in 2013, is eight shots off the lead after four birdies in a 68.</s>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0][batch['attention_mask'][0]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1566,   348, 12020,  7459,  6591,  3138,    16,    21,     3,     9,\n",
       "         5963,    23,    15,    44,     8,   804,  6356,    12,   698,     8,\n",
       "        22653,   991,    44,     8,   391,  7645, 11523,    44, 22003,  3642,\n",
       "           16,  1013,  5089,     5,     1,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Englishman Luke Donald chipped in for a birdie at the final hole to share the halfway lead at the RBC Heritage at Hilton Head in South Carolina.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch['labels'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2022",
   "language": "python",
   "name": "pytorch2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
